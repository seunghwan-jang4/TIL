{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 사용환경 준비\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key 입력: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 모델 준비\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 문서 로드하기\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력 / 인공지능 산업의 최신동향.pdf\n",
    "loader = PyPDFLoader(\"C:/Users/82103/Desktop/과제파일/인공지능 산업의 최신동향.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **청킹 방식**\n",
    "1. 첫 번째 청킹 방식 : CharacterTextSplitter: 텍스트를 단순히 지정한 크기(chunk_size)로 자른다.  \n",
    "2. 두 번째 청킹 방식 : RecursiveCharacterTextSplitter: 텍스트를 더 자연스럽게 나누기 위해 문장의 구조를 고려하며 점진적으로 쪼갠다.  \n",
    "\n",
    "\n",
    "#### **청킹의 각 parameter의 뜻**\n",
    "\n",
    "1. chunk_size : 한 청크의 최대 길이.\n",
    "2. chunk_overlap : 청크 간의 겹치는 문자 수. 컨텍스트 연속성을 유지하기 위해 사용. (청크의 끝부분이 다음 청크의 시작부분과 겹치도록 한다.)\n",
    "3. length_function : 청크의 길이를 측정.\n",
    "4. separator : 텍스트를 분할하는 기준.\n",
    "5. is_separator_regex : 말 그대로 separator를 정규식으로 사용할지 여부. True로 설정하면, 정규식으로 인식. False로 설정하면, 단순 문자열로 인식.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. 문서 청크로 나누기 / 청킹된 내용을 상위 10개까지 출력, 각 청킹방식과 parameter의 뜻을 markdown으로 정리.\n",
    "#첫 번째 청킹 방식 : CharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. 문서 청크로 나누기 / 청킹된 내용을 상위 10개까지 출력, 각 청킹방식과 parameter의 뜻을 markdown으로 정리.\n",
    "#두 번째 청킹 방식 : RecursiveCharacterTextSplitter \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. 벡터 임베딩 생성\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. 벡터 스토어 생성\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. FAISS를 Retriever로 변환\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. 프롬프트 템플릿 정의 \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82103\\AppData\\Local\\Temp\\ipykernel_5872\\1373480811.py:40: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  llm_chain = LLMChain(llm=model, prompt=contextual_prompt)\n"
     ]
    }
   ],
   "source": [
    "#9. RAG 체인 구성\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "class SimplePassThrough:\n",
    "    def invoke(self, inputs, **kwargs):\n",
    "        return inputs\n",
    "\n",
    "class ContextToPrompt:\n",
    "    def __init__(self, prompt_template):\n",
    "        self.prompt_template = prompt_template\n",
    "    \n",
    "    def invoke(self, inputs):\n",
    "        # 문서 내용을 텍스트로 변환\n",
    "        if isinstance(inputs, list):\n",
    "            context_text = \"\\n\".join([doc.page_content for doc in inputs])\n",
    "        else:\n",
    "            context_text = inputs\n",
    "        \n",
    "        # 프롬프트 템플릿에 적용\n",
    "        formatted_prompt = self.prompt_template.format_messages(\n",
    "            context=context_text,\n",
    "            question=inputs.get(\"question\", \"\")\n",
    "        )\n",
    "        return formatted_prompt\n",
    "\n",
    "# Retriever를 invoke() 메서드로 래핑하는 클래스 정의\n",
    "class RetrieverWrapper:\n",
    "    def __init__(self, retriever):\n",
    "        self.retriever = retriever\n",
    "\n",
    "    def invoke(self, inputs):\n",
    "        if isinstance(inputs, dict):\n",
    "            query = inputs.get(\"question\", \"\")\n",
    "        else:\n",
    "            query = inputs\n",
    "        # 검색 수행\n",
    "        response_docs = self.retriever.get_relevant_documents(query)\n",
    "        return response_docs\n",
    "\n",
    "llm_chain = LLMChain(llm=model, prompt=contextual_prompt)\n",
    "\n",
    "# RAG 체인 설정\n",
    "rag_chain_debug = {\n",
    "    \"context\": RetrieverWrapper(retriever),\n",
    "    \"prompt\": ContextToPrompt(contextual_prompt),\n",
    "    \"llm\": model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RAG의 필요성**\n",
    "1. 외부 데이터베이스나 검색 시스템으로 최신 정보와 특정 주제에 대한 답을 생성. 검색하고 참고하여 생성하기에, \"__정확성과 신뢰도__\" 증가.\n",
    "2. 필요한 정보만 실시간 검색하여 사용. 따라서, 모델 크기를 줄이고 \"__메모리 자원을 효율적__\" 으로 활용 가능.\n",
    "3. 데이터를 동적으로 연결하여, 언어 모델을 \"__특정 도메인에 최적화__\" 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. 챗봇 구동 확인 / 같은 질문을 일반 chat gpt와 답변을 비교해보고, 왜 RAG이 필요한지 간단히 markdown으로 서술.\n",
    "# 챗봇 구동\n",
    "while True:\n",
    "    print(\"========================\")\n",
    "    query = input(\"질문을 입력하세요 : \")\n",
    "    \n",
    "    # 1. Retriever로 관련 문서 검색\n",
    "    response_docs = rag_chain_debug[\"context\"].invoke({\"question\": query})\n",
    "    \n",
    "    # 2. 문서를 프롬프트로 변환\n",
    "    prompt_messages = rag_chain_debug[\"prompt\"].invoke({\n",
    "        \"context\": response_docs,\n",
    "        \"question\": query\n",
    "    })\n",
    "    \n",
    "    # 3. LLM으로 응답 생성\n",
    "    response = rag_chain_debug[\"llm\"].invoke(prompt_messages)\n",
    "    \n",
    "    print(\"\\n답변:\")\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **질문**\n",
    "\n",
    "\"최신 인공지능의 동향을 알려줘\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **챗봇의 답변**\n",
    "\n",
    "최신 인공지능 동향에 대한 구체적인 내용은 제공된 문서의 2페이지에 위치해 있습니다. 문서에서는 인공지능 산업 동향에 대한 브리프를 다루고 있습니다. 더 자세한 정보를 원하신다면 해당 문서를 참고하시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ChatGPT의 답변**\n",
    "\n",
    "#사이트 5개 검색함\n",
    "\n",
    "\n",
    "2024년의 인공지능(AI)은 다양한 기술적 발전과 응용으로 큰 주목을 받고 있습니다. 주요 동향은 다음과 같습니다.\n",
    "\n",
    "1. 생성형 AI와 다중 모달 시스템\n",
    "생성형 AI는 기존 텍스트 중심에서 이미지, 오디오, 동영상 데이터를 포함하는 다중 모달 AI 시스템으로 진화하고 있습니다. 이 시스템은 인간처럼 다양한 감각 데이터를 통합해 문제를 해결할 수 있으며, 자동차의 피로 감지, 보험 클레임 관리 등 여러 산업에서 활용됩니다​\n",
    "NVIDIA BLOG KOREA\n",
    "​\n",
    "고품질 학습데이터가 AI 모델의 성공을 만듭니다 | APPEN 에펜\n",
    ".\n",
    "2. AI와 과학의 융합\n",
    "생명과학과 재료 과학 분야에서 AI는 혁신적인 연구 속도를 크게 높이고 있습니다. 예를 들어, 암 치료에 사용되는 대규모 AI 모델과 신약 개발을 위한 AI 기술이 수년 걸리던 과정을 수개월로 단축하고 있습니다​\n",
    "인공지능신문\n",
    ".\n",
    "3. AI 윤리와 규제\n",
    "AI 활용이 확산됨에 따라, 데이터 프라이버시, 투명성, 그리고 책임성을 강화하기 위한 윤리적 기준과 규제 프레임워크가 강화되고 있습니다. NIST AI 위험 관리 프레임워크와 같은 표준이 대규모 기업에서 AI 채택을 촉진하고 있습니다​\n",
    "추천 : 네이버 블로그\n",
    "​\n",
    "고품질 학습데이터가 AI 모델의 성공을 만듭니다 | APPEN 에펜\n",
    ".\n",
    "4. AI의 상업적 활용 증가\n",
    "기업들은 고객 맞춤형 응용 프로그램과 지능형 비즈니스 도구를 제공하기 위해 AI API를 통합하고 있습니다. 이와 함께, 기업 데이터에 최적화된 RAG(Retrieval-Augmented Generation) 기술이 부정확한 정보 생성을 줄이고 있습니다​\n",
    "NVIDIA BLOG KOREA\n",
    "​\n",
    "추천 : 네이버 블로그\n",
    ".\n",
    "5. AI와 지속가능성\n",
    "AI는 농업과 환경 보전에서도 두각을 나타냅니다. 예를 들어, 기후변화 완화를 위한 일기예보 시스템과 농업 생산성을 높이는 도구가 개발되고 있습니다​\n",
    "인공지능신문\n",
    ".\n",
    "결론\n",
    "2024년은 AI 기술의 진화가 산업과 일상생활에 깊숙이 스며들면서, 도전과 기회를 동시에 제공하는 해가 될 것으로 보입니다. 이를 기반으로 스마트 도시, 의료 혁신, 그리고 지속 가능한 환경을 구축할 가능성이 열리고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **챗봇과 ChatGPT의 답변 비교**\n",
    "\n",
    "둘의 내용을 비교하여 봤을 때, 기본적으로 챗봇은 현재 제공된 문서에 기반하여 정보를 전달해 준다.\n",
    "\n",
    "반면에 ChatGPT의 경우는, 여러 사이트를 검색한 모습을 명시적으로 확인할 수 있었다. 그래서 그에 따른 정보들도 신뢰와 정확도 측면에서 더 강화된 느낌을 받았다.\n",
    "위에 짧게 언급한 부분을 쉽고 빠르게 체감하니 RAG의 필요성을 느끼기에 충분했다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
